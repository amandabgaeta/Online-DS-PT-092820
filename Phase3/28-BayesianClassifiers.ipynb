{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are using an API to gather articles from a news website and grabbing phrases from two different types of articles: on **sports** and on **politics**.\n",
    "\n",
    "Is there a way we can use machine learning to help us label the articles quickly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports = ['the match was close',\n",
    "          'the coaches agreed on strategy',\n",
    "          'played in a sold out stadium']\n",
    "\n",
    "politics = ['world leaders met last week',\n",
    "            'the election was close',\n",
    "            'the officials agreed on a compromise']\n",
    "\n",
    "test_statement = 'world leaders agreed to fund the stadium'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing Back Bayes\n",
    "\n",
    "> \"Naive Bayes classifiers are linear classifiers that are known for being **simple yet very efficient**. The probabilistic model of naive Bayes classifiers is based on Bayes’ theorem, and the adjective naive comes from the assumption that the features in a dataset are **mutually independent**. In practice, the independence assumption is often violated, but naive Bayes classifiers **still tend to perform very well** under this unrealistic assumption. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives.\"\n",
    "\n",
    "[Source: Sebasitian Raschka: Naive Bayes and Text Classification](https://sebastianraschka.com/Articles/2014_naive_bayes_1.html) (emphasis is mine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisiting the theorem itself:\n",
    "\n",
    "![bayes theorem](images/bayes_theorem.svg)\n",
    "\n",
    "#### AKA\n",
    "\n",
    "![breaking down the function behind naive bayes](images/naive_bayes_icon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Another way of looking at it\n",
    "\n",
    "![further breakdown of the pieces of naive bayes](images/another_one.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, in the context of our problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## $ P(politics | document) = \\frac{P(document|politics)P(politics)}{P(document)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to calculate each piece in turn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we calculate $ P(politics) $ ?\n",
    "\n",
    "This is essentially the distribution of the probability of either type of article. We have three of each type of article in our current set of example documents, therefore, we assume that there is an equal probability of either type.\n",
    "\n",
    "\n",
    "## $ P(politics) = \\frac{\\# politics\\ documents}{\\# all\\ documents} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['world leaders met last week',\n",
       " 'the election was close',\n",
       " 'the officials agreed on a compromise']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the match was close',\n",
       " 'the coaches agreed on strategy',\n",
       " 'played in a sold out stadium']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can check this, though\n",
    "# going back to our intro example data...\n",
    "p_politics = len(politics)/(len(politics) + len(sports))\n",
    "p_politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sports = len(sports)/(len(politics) + len(sports))\n",
    "p_sports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we calculate $ P(document | politics) $ ?\n",
    "\n",
    "We need to break the phrases down into individual words - with the hope that these words actually tell us more, since we likely have never seen this exact document before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ P(phrase | politics) = \\prod_{i=1}^{d} P(word_{i} | politics) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ P(word_{i} | politics) = \\frac{\\#\\ of\\ word_{i}\\ in\\ politics\\ docs} {\\#\\ of\\ total\\ words\\ in\\ politics\\ docs} $\n",
    "\n",
    "#### Can you foresee any issues with this?\n",
    "\n",
    "- If you see a word in your test set that wasn't in your training set, you will get and multiply by ZEROs which will make the math not work properly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter: Laplace Smoothing\n",
    "\n",
    "### $ P(word_{i} | politics) = \\frac{\\#\\ of\\ word_{i}\\ in\\ politics\\ docs + \\alpha} {\\#\\ of\\ total\\ words\\ in\\ politics\\ docs + \\alpha d} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correction process is called Laplace Smoothing:\n",
    "\n",
    "- d : number of features (in this instance total number of vocabulary words)\n",
    "- $\\alpha$ can be any number greater than 0 (it is usually 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we calculate $ P(document) $ ?\n",
    "\n",
    "- well... we don't have to, because the P(document) doesn't change whether we're looking at sports or politics, we're just going to compare the numerator of these to see which is bigger between sports or politics!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So why is this 'naive' ?\n",
    "\n",
    "> \"Naive Bayes (NB) is ‘naive’ because it makes the assumption that features of a measurement are independent of each other. This is naive because it is (almost) never true.\"\n",
    "\n",
    "[Source - 'What's So Naive About Naive Bayes?', a Towards Data Science blog post all about this](https://towardsdatascience.com/whats-so-naive-about-naive-bayes-58166a6a9eba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's calculate this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the match was close', 'the coaches agreed on strategy', 'played in a sold out stadium']\n",
      "['world leaders met last week', 'the election was close', 'the officials agreed on a compromise']\n"
     ]
    }
   ],
   "source": [
    "print(sports)\n",
    "print(politics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| word       | frequency in politics | frequency in sports |\n",
    "| ---------- | --------------------- | ------------------- |\n",
    "| the        |  2                    | 2                   |\n",
    "| match      |  0                    | 1                   |\n",
    "| was        |  1                    | 1                   |\n",
    "| close      |  1                    | 1                   |\n",
    "| coaches    |  0                    | 1                   |\n",
    "| agreed     |  1                    | 1                   |\n",
    "| on         |  1                    | 1                   |\n",
    "| strategy   |  0                    | 1                   |\n",
    "| played     |  0                    | 1                   |\n",
    "| in         |  0                    | 1                   |\n",
    "| a          |  1                    | 1                   |\n",
    "| sold       |  0                    | 1                   |\n",
    "| out        |  0                    | 1                   |\n",
    "| stadium    |  0                    | 1                   |\n",
    "| world      |  1                    | 0                   |\n",
    "| leaders    |  1                    | 0                   |\n",
    "| met        |  1                    | 0                   |\n",
    "| last       |  1                    | 0                   |\n",
    "| week       |  1                    | 0                   |\n",
    "| election   |  1                    | 0                   |\n",
    "| officials  |  1                    | 0                   |\n",
    "| compromise |  1                    | 0                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Test sentence: 'world leaders agreed to fund the stadium'\n",
    "\n",
    "| word    | $ P( word | politics) $                | $ P( word | sports) $                  |\n",
    "| ------- | -------------------------------------- | -------------------------------------- |\n",
    "| world   | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| leaders | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| agreed  | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ |\n",
    "| to      | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| fund    | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| the     | $\\frac{2 + 1}{15 + 30} = \\frac{3}{45}$ | $\\frac{2 + 1}{15 + 30} = \\frac{3}{45}$ |\n",
    "| stadium | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dunno about you... but I'm already exhausted trying to do this from scrach, and that's just a single sentence. Let's move into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we want\n",
    "\n",
    "$ P(document|sports) * P(sports) $ **vs.** $ P(document|politics) * P(politics) $\n",
    "\n",
    "We already calculated $P(sports)$ and $P(politics)$ above - they're both 50% since we have an equal number of documents in each. But the $P(document | label)$ for the two labels is going to require breaking out each word to find the likelihood that each word is in each category (plus Laplacian smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_maker(category):\n",
    "    \"\"\"\n",
    "    returns the vocabulary for a given type of article\n",
    "    \"\"\"\n",
    "    vocab_category = set()\n",
    "    for art in category:\n",
    "        words = art.split()\n",
    "        for word in words:\n",
    "            vocab_category.add(word)\n",
    "    return vocab_category\n",
    "        \n",
    "voc_sports = vocab_maker(sports)\n",
    "voc_pol = vocab_maker(politics)\n",
    "total_vocabulary = voc_sports.union(voc_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'agreed',\n",
       " 'close',\n",
       " 'coaches',\n",
       " 'in',\n",
       " 'match',\n",
       " 'on',\n",
       " 'out',\n",
       " 'played',\n",
       " 'sold',\n",
       " 'stadium',\n",
       " 'strategy',\n",
       " 'the',\n",
       " 'was'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'agreed',\n",
       " 'close',\n",
       " 'compromise',\n",
       " 'election',\n",
       " 'last',\n",
       " 'leaders',\n",
       " 'met',\n",
       " 'officials',\n",
       " 'on',\n",
       " 'the',\n",
       " 'was',\n",
       " 'week',\n",
       " 'world'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'agreed',\n",
       " 'close',\n",
       " 'coaches',\n",
       " 'compromise',\n",
       " 'election',\n",
       " 'in',\n",
       " 'last',\n",
       " 'leaders',\n",
       " 'match',\n",
       " 'met',\n",
       " 'officials',\n",
       " 'on',\n",
       " 'out',\n",
       " 'played',\n",
       " 'sold',\n",
       " 'stadium',\n",
       " 'strategy',\n",
       " 'the',\n",
       " 'was',\n",
       " 'week',\n",
       " 'world'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_count = len(total_vocabulary) # useful for laplacian smoothing\n",
    "total_sports_count = len(voc_sports)\n",
    "total_politics_count = len(voc_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number_words_in_category(phrase, category):\n",
    "    '''\n",
    "    returns number of words in the phrase previously found in the category\n",
    "    \n",
    "    inputs:\n",
    "    phrase - string, test phrase to classify\n",
    "    category - list, all training phrases associated with that category\n",
    "    \n",
    "    output:\n",
    "    word_count - default dictionary, with each word in the phrase as a key \n",
    "                 with a value of the number of times the words have \n",
    "                 appeared in the category in the train set\n",
    "    '''\n",
    "    # gets each word out - statement is a list object now\n",
    "    statement = phrase.split()\n",
    "    \n",
    "    # creating one big string from the provided category list\n",
    "    str_category=' '.join(category)\n",
    "    # splitting now so it's a single list of the words found in the category\n",
    "    cat_word_list = str_category.split()\n",
    "    # default dict allows us to create new keys easily\n",
    "    word_count = defaultdict(int) \n",
    "    \n",
    "    for word in statement:\n",
    "        for cat_word in cat_word_list:\n",
    "            if word == cat_word:\n",
    "                word_count[word] +=1\n",
    "            else:\n",
    "                word_count[word] # here's the part that works because default dict\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'world': 0,\n",
       "             'leaders': 0,\n",
       "             'agreed': 1,\n",
       "             'to': 0,\n",
       "             'fund': 0,\n",
       "             'the': 2,\n",
       "             'stadium': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sports_word_count = find_number_words_in_category(test_statement,sports)\n",
    "test_sports_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'world': 1,\n",
       "             'leaders': 1,\n",
       "             'agreed': 1,\n",
       "             'to': 0,\n",
       "             'fund': 0,\n",
       "             'the': 2,\n",
       "             'stadium': 0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_politic_word_count = find_number_words_in_category(test_statement,politics)\n",
    "test_politic_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ P(politics | article) = P(politics) x \\prod_{i=1}^{d} P(word_{i} | politics) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_likelihood(category_count, test_category_count, alpha):\n",
    "    \n",
    "    num = np.product(np.array(list(test_category_count.values())) + alpha)\n",
    "    denom = (category_count + total_vocab_count*alpha)**(len(test_category_count))\n",
    "    \n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "likelihood_sports = find_likelihood(total_sports_count,test_sports_word_count,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_politics = find_likelihood(total_politics_count,test_politic_word_count,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5313121933259447e-10\n",
      "3.0626243866518893e-10\n"
     ]
    }
   ],
   "source": [
    "# yeah... the probabilities out don't mean anything, just worry about which is bigger\n",
    "print(likelihood_sports)\n",
    "print(likelihood_politics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determing the winner of our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/solvingforyhat.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p(politics|article) > p(sports|article)\n",
    "(likelihood_politics * p_politics) > (likelihood_sports * p_sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros:\n",
    "\n",
    "* It is an efficient way to predict class of test data set. It perform well in multi class prediction\n",
    "* When assumption of independence holds, a Naive Bayes classifier performs requires less training data and can perform better than models like logistic regression.\n",
    "* Performs better with categorical inputs. For numerical input, one has to assume a normal distribution.\n",
    "\n",
    "### Cons:\n",
    "\n",
    "* Naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously\n",
    "* We are assuming of independent predictors, but in real life, it is almost impossible that we get a set of predictors which are completely independent (amazingly, still works a lot of the time though!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but let's be real, we don't need to use hand-written functions for this\n",
    "\n",
    "### Using Naive Bayes in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more imports\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# fetching our data\n",
    "news_train = fetch_20newsgroups(subset='train', \n",
    "                                categories = ['rec.sport.baseball', \n",
    "                                              'talk.politics.misc'])\n",
    "news_test = fetch_20newsgroups(subset='test', \n",
    "                               categories = ['rec.sport.baseball', \n",
    "                                              'talk.politics.misc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting data in dataframe\n",
    "df_train = pd.DataFrame()\n",
    "df_train['Data'] = news_train.data\n",
    "df_train['Target'] = news_train.target\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['Data'] = news_test.data\n",
    "df_test['Target'] = news_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'rec.sport.baseball', 1: 'talk.politics.misc'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grabbing our target classes so we know which is which\n",
    "target_classes = dict(enumerate(news_test.target_names))\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1062 entries, 0 to 1061\n",
      "Data columns (total 2 columns):\n",
      "Data      1062 non-null object\n",
      "Target    1062 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 16.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: pcaster@mizar.usc.edu (Dodger)\\nSubject:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: lbr@holos0.uucp (Len Reed)\\nSubject: Re:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: scott@asd.com (Scott Barman)\\nSubject: R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Organization: City University of New York\\nFro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: dos@major.panix.com (Dave O'Shea)\\nSubje...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data  Target\n",
       "0  From: pcaster@mizar.usc.edu (Dodger)\\nSubject:...       0\n",
       "1  From: lbr@holos0.uucp (Len Reed)\\nSubject: Re:...       0\n",
       "2  From: scott@asd.com (Scott Barman)\\nSubject: R...       0\n",
       "3  Organization: City University of New York\\nFro...       1\n",
       "4  From: dos@major.panix.com (Dave O'Shea)\\nSubje...       1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.info()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 707 entries, 0 to 706\n",
      "Data columns (total 2 columns):\n",
      "Data      707 non-null object\n",
      "Target    707 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 11.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: jaffray@dent.uchicago.edu (Alan Jaffray)...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: as010b@uhura.cc.rochester.edu (Tree of S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: reed5575@iscsvax.uni.edu\\nSubject: Re: B...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: sys1@exnet.co.uk (Xavier Gallagher)\\nSub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: carroll@hercules.cis.udel.edu (Mark C. C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data  Target\n",
       "0  From: jaffray@dent.uchicago.edu (Alan Jaffray)...       1\n",
       "1  From: as010b@uhura.cc.rochester.edu (Tree of S...       1\n",
       "2  From: reed5575@iscsvax.uni.edu\\nSubject: Re: B...       0\n",
       "3  From: sys1@exnet.co.uk (Xavier Gallagher)\\nSub...       1\n",
       "4  From: carroll@hercules.cis.udel.edu (Mark C. C...       1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.info()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Target Ratio: 0.4379\n",
      "Test Target Ratio: 0.4385\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Target Ratio: {df_train[\"Target\"].mean():.4f}')\n",
    "print(f'Test Target Ratio: {df_test[\"Target\"].mean():.4f}')\n",
    "# roughly equivalent breakdowns between classes in train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: ipser@solomon.technet.sg (Ed Ipser)\\nSubject: Government-Mandated Energy Conservation is Unnecessary and Wastful, Study Finds\\nNntp-Posting-Host: solomon.technet.sg\\nLines: 94\\n\\n\\n\\n  Government-Mandated Energy Conservation is Unnecessary and Wastful, Study Finds\\n\\n  Washington, DC -- The energy tax and subsidized energy-efficiency\\n  measures supported by President Clinton and Energy Secretary Hazel\\n  O\\'Leary are based on faulty assumptions, a new study from the Cato\\n  Institute points out.\\n\\n    According to Jerry Taylor, Cato\\'s director of natural resource studies,\\n  we are not running out of sources of energy. The world now has almost 10\\n  times the proven oil reserves it had in 1950 and twice the reserves of\\n  1970. Proven reserves of coal and natural gas have increased just as\\n  dramatically.\\n\\n    When standards of living, population densities, and industrial\\n  structures are controlled for, the United States is no less energy\\n  efficient than Japan and more energy efficient than many of the Group\\n  of Seven nations.\\n\\n    Energy independence provides little protection against domestic oil\\n  price shocks because the energy economy is global. Moreover, since the\\n  cost of oil represents only about 2 percent of gross national product,\\n  even large increases in the price of oil would have little impact on the\\n  overall U.S. economy.\\n\\n    Market economies are, on average, 2.75 times more energy efficient per\\n  $1,000 of GNP than are centrally planned economies.\\n\\n    Utilities\\' subsidized energy-efficiency measurs, known as demand-side\\n  management programs, encourage free riders, overuse of competing resource\\n  inputs, an competitive inequities. Furthermore, DSM programs do not\\n  reduce demand.\\n\\n    Taylor concludes that government-mandated energy conservation imposes\\n  unnecessary costs on consumers and wastes, not conserves, energy; that\\n  subsidizing energy-conservation technologies will stymie, not advance,\\n  gains in energy conservation; and that central control over the lifeblood\\n  of modern society--energy--would transfer tremendous power to the state\\n  at the expense of the individual.\\n\\n    \"Energy Conservation and Efficiency: The Case Against Coercion\" is no.\\n  189 in the Policy Analysis series published by the Cato Institute, an\\n  independent public policy research organization in Washington, DC.\\n\\n\\n\\nAvailable from:\\n  Cato Institute\\n  224 Second Street SE\\n  Washington, DC  20003\\n\\n\\n\\n---------------------------------------------------------------------------\\n\\n\\n\\n\\n                        The Cato Institute\\n\\n    Founded in 1977, the Cato Institute is a public policy research\\n  foundation dedicated to broadening the parameters of policy debate\\n  to allow consideration of more options that are consistent with the\\n  traditional American principles of limited government, individual\\n  liberty, and peace.  To that end, the Institute strives to achieve\\n  greater involvement of the intelligent, concerned lay public in \\n  questions of policy and the proper role of government.\\n    The Institute is named for Cato\\'s Letters, libertarian pamphlets\\n  that were widely read in the American Colonies in the early 18th\\n  century and played a major role in laying the philosophical foundation\\n  of the American Revolution.\\n    Despite the achievement of the nation\\'s Founders, today virtually\\n  no aspect of life is free from government encroachment.  A pervasive\\n  intolerance for individual rights is shown by government\\'s arbitrary\\n  intrusions into private economic transactions and its disregard for\\n  civil liberties.\\n    To counter that trend the Cato Institute undertakes an extensive\\n  publications program that addresses the complete spectrum of policy\\n  issues.  Books, monographs, and shorter studies are commissioned\\n  to examine the federal budget, Social Security, regulation, military\\n  spending, international trade, and myriad other issues.  Major policy\\n  conferences are held throughout the year, from which papers are\\n  published thrice yearly in the Cato Journal.\\n    In order to maintain its independence, the Cato Institute accepts\\n  no government funding.  Contributions are received from foundations,\\n  corporations, and individuals, and other revenue is generated from\\n  the sale of publications.  The Institute is a nonprofit, tax-exempt,\\n  educational foundation under Section 501(c)3 of the Internal Revenue\\n  Code.\\n\\n  The Cato Institute\\n  224 Second Street S.E.\\n  Washington, DC  20003\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Data'][950]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need to turn our text data into numbers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a Count Vectorizer\n",
    "# Goes through each doc and counts how many of each word\n",
    "vectorizer = CountVectorizer()\n",
    "# Fitting and transforming our train data\n",
    "X_train = vectorizer.fit_transform(df_train['Data']).toarray() # to array is just for the model later\n",
    "# Just transforming our test data\n",
    "X_test = vectorizer.transform(df_test['Data']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000007</th>\n",
       "      <th>000k</th>\n",
       "      <th>000th</th>\n",
       "      <th>0010</th>\n",
       "      <th>001116</th>\n",
       "      <th>001211</th>\n",
       "      <th>001338</th>\n",
       "      <th>002</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooid</th>\n",
       "      <th>zorba</th>\n",
       "      <th>zumwalt</th>\n",
       "      <th>zupcic</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzzzzz</th>\n",
       "      <th>zzzzzzt</th>\n",
       "      <th>ñaustin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19776 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  000007  000k  000th  0010  001116  001211  001338  002  ...  zone  \\\n",
       "0   0    0       0     0      0     0       0       0       0    0  ...     0   \n",
       "1   0    0       0     0      0     0       0       0       0    0  ...     0   \n",
       "2   0    0       0     0      0     0       0       0       0    0  ...     0   \n",
       "3   0    0       0     0      0     0       0       0       0    0  ...     0   \n",
       "4   0    0       0     0      0     0       0       0       0    0  ...     0   \n",
       "\n",
       "   zoo  zooid  zorba  zumwalt  zupcic  zz  zzzzzz  zzzzzzt  ñaustin  \n",
       "0    0      0      0        0       0   0       0        0        0  \n",
       "1    0      0      0        0       0   0       0        0        0  \n",
       "2    0      0      0        0       0   0       0        0        0  \n",
       "3    0      0      0        0       0   0       0        0        0  \n",
       "4    0      0      0        0       0   0       0        0        0  \n",
       "\n",
       "[5 rows x 19776 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does this look like?\n",
    "X_train_vectorized = pd.DataFrame(X_train, columns=vectorizer.get_feature_names())\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explore a single example of our new vectorized X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: pcaster@mizar.usc.edu (Dodger)\\nSubject:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data  Target\n",
       "0  From: pcaster@mizar.usc.edu (Dodger)\\nSubject:...       0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before\n",
    "df_train.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: pcaster@mizar.usc.edu (Dodger)\\nSubject: Gross Grosses Out Dodger Fans AGAIN.\\nOrganization: University of Southern California, Los Angeles, CA\\nLines: 60\\nNNTP-Posting-Host: mizar.usc.edu\\n\\nWent to the Dodgers game tonight -- it was cap night.\\n \\nAstacio pitched ok, but had control trouble all night.\\nIn the first, he walked a batter, balked him to second, then\\na single scored the run, with the batter taking second on the\\nthrow home.  Another single made the score 2-0 Cards.\\n \\nLasorda tried a new line up featuring Butler, Reed, And Piazza\\nbatting third!  Darryl and Eric were benched in favor of Snyder\\nand Webster.\\n \\nPiazza homered in the first to make the score 2-1 Cards.\\nThe Dodgers tied the game in the second on a two out single\\nby Offerman.\\n \\nBy the fourth inning, Astacio had already made about 80 pitches, but\\nthe score was still 2-2.  The Dodger defense made SEVERAL impressive\\nplays.  Piazza looked GREAT behind the plate, gunning down a runner\\ntrying to steal second, throwing a runner out at first who\\nhad strayed a bit from first base, etc.\\n \\nKarros also made a spectacular play, keeping a ball from going into\\nthe outfield.  The runner on first was so sure that ball was going\\nthrough, he just kept running past second.  Karros got up and threw\\nto third and EASILY got the runner at third.\\n \\nMy heart sank in the 7th when Gross got up to warm up in the bullpen.\\n \\nAstacio was lifted for a pinch hitter, and when Gross entered the game\\nwith the score still 2-2, Dodger fans just KNEW it was over.\\n \\nGross was relieving because he stunk on Tuesday, pitching just 2 1/3\\ninnings, forcing Lasorda to use much of his bullpen.  The 15 inning\\ngame had the same effect the next night...so only Gross was fresh\\ngiven his light work out Tuesday.\\n \\nGross lived up to his name.  He walked the first batter, gave up a hit\\nto the second, and walked the bases loaded.  After a grounder resulted\\nin a force at home, Zeile lifted a scoring fly ball to make it\\n3-2 Cards.  Gross paid little attention to the runners, and the next\\nthing you knew, the Cards had stolen a fourth run.  The runner on\\nfirst was eventually tagged out in the run down, but the 4th run had\\nscored long before that.\\n \\nMeanwhile, the Dodgers mounted little offense after the second inning.\\nLee Smith pitched the ninth.  He had little trouble getting Karros\\nand Wallach (does anyone have trouble with Wallach these days?).\\nCory Snyder collected his first hit as a Dodger, a single, but\\nthat was all the offense the Dodgers could mount.  Smith got his\\nthird straight save against the Dodgers and all I got was my\\nfree Dodger cap and a good look at Piazza.  If Piazza keeps this\\nup all year, he will be a strong candidate for rookie of the year\\nhonors.  Though its really early, Karros is already showing signs\\nof a sophomore jinx year.\\n \\nThe final score...Cardinals 4 runs on 7 hits.\\nDodgers 2 runs on 7 hits.\\n \\nDodger\\n\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full text before\n",
    "df_train['Data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the        42\n",
       "was        11\n",
       "to         11\n",
       "and        11\n",
       "in          8\n",
       "first       8\n",
       "dodger      7\n",
       "on          7\n",
       "gross       7\n",
       "up          7\n",
       "had         7\n",
       "second      7\n",
       "he          6\n",
       "dodgers     6\n",
       "piazza      5\n",
       "score       5\n",
       "his         5\n",
       "got         5\n",
       "runner      5\n",
       "out         5\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After\n",
    "X_train_vectorized.iloc[0].sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now time to model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting our y values\n",
    "y_train = df_train['Target']\n",
    "y_test = df_test['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Test Accuracy: 0.9576\n",
      "Naive Bayes Test F1-Score: 0.9524\n"
     ]
    }
   ],
   "source": [
    "# Instantiating our model - just using default values\n",
    "model = GaussianNB()\n",
    "# Fitting our model\n",
    "model.fit(X_train,y_train)\n",
    "# Making predictions on our test set\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# How'd we do? Super well\n",
    "print(f'Naive Bayes Test Accuracy: {accuracy_score(y_test, y_preds):.4f}')\n",
    "print(f'Naive Bayes Test F1-Score: {f1_score(y_test, y_preds):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-554e10d6b4f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(model, X_test, y_test, display_labels = target_classes.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Accuracy: 0.9434\n",
      "Logistic Regression Test F1-Score: 0.9349\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4eeb6a0d5b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Logistic Regression Test F1-Score: {f1_score(y_test, y_preds_lr):.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# for comparison...\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiating our model - just using default values\n",
    "logreg = LogisticRegression(random_state=123)\n",
    "# Fitting our model\n",
    "logreg.fit(X_train,y_train)\n",
    "# Making predictions on our test set\n",
    "y_preds_lr = logreg.predict(X_test)\n",
    "\n",
    "# How'd we do?\n",
    "print(f'Logistic Regression Test Accuracy: {accuracy_score(y_test, y_preds_lr):.4f}')\n",
    "print(f'Logistic Regression Test F1-Score: {f1_score(y_test, y_preds_lr):.4f}')\n",
    "\n",
    "plot_confusion_matrix(logreg, X_test, y_test, display_labels = target_classes.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuned Random Forest Test Accuracy: 0.8939\n",
      "Untuned Random Forest Test F1-Score: 0.8705\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-41bb6725324a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Untuned Random Forest Test F1-Score: {f1_score(y_test, y_preds_rf):.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# another comparison...\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiating our model - just using default values\n",
    "rf = RandomForestClassifier(random_state=123)\n",
    "# Fitting our model\n",
    "rf.fit(X_train,y_train)\n",
    "# Making predictions on our test set\n",
    "y_preds_rf = rf.predict(X_test)\n",
    "\n",
    "# How'd we do?\n",
    "print(f'Untuned Random Forest Test Accuracy: {accuracy_score(y_test, y_preds_rf):.4f}')\n",
    "print(f'Untuned Random Forest Test F1-Score: {f1_score(y_test, y_preds_rf):.4f}')\n",
    "\n",
    "plot_confusion_matrix(rf, X_test, y_test, display_labels = target_classes.values())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
